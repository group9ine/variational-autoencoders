{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca5df7a",
   "metadata": {},
   "source": [
    "### NOTEBOOK 1\n",
    "# Variational Auto Encoder\n",
    "### Architecture and training\n",
    "\n",
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09619091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import linalg as la\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    Input,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Lambda,\n",
    "    Reshape,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec146f4",
   "metadata": {},
   "source": [
    "Define core features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e912302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "numpart = 30\n",
    "latent_dim = 40\n",
    "box_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feada97c",
   "metadata": {},
   "source": [
    "Import the training data and add gamma labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece32d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dir = \"./\"\n",
    "# read all position files in chosen directory\n",
    "files = glob(dump_dir + \"gamma*_x.txt\")\n",
    "# sort files by gamma value\n",
    "files = np.array(files)[np.argsort([f.split(\"_\")[1] for f in files])]\n",
    "gamma = np.sort([f.split(\"_\")[1] for f in files]).astype(float)\n",
    "\n",
    "num_gammas = files.size\n",
    "arrays = [np.loadtxt(f) for f in files]\n",
    "\n",
    "# combine data + reshape, and assign labels to different datasets\n",
    "data = np.vstack(arrays)\n",
    "vcs = data.reshape((-1, numpart, dim)) / (box_size * np.sqrt(dim))\n",
    "labels = np.hstack([[i] * len(a) for i, a in enumerate(arrays)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a57bd2",
   "metadata": {},
   "source": [
    "### Manipulate data\n",
    "Sort points by distance from origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53aea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(vcs[:, :, 0] ** 2 + vcs[:, :, 1] ** 2)\n",
    "sorted_vcs = np.array(\n",
    "    [sample[sort_idx[i]] for i, sample in enumerate(vcs)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922957da",
   "metadata": {},
   "source": [
    "Compute distance matrix for each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = np.array([squareform(pdist(sample)) for sample in sorted_vcs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184645c",
   "metadata": {},
   "source": [
    "Split in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = 0.8\n",
    "\n",
    "m = sorted_vcs.shape[0]  # total number of samples\n",
    "m_training = int(m * train_perc)  # samples in the training set\n",
    "m_test = m - m_training  # samples in the test set\n",
    "\n",
    "while True:\n",
    "    permutation = np.random.permutation(m)\n",
    "\n",
    "    sorted_vcs = sorted_vcs[permutation]\n",
    "    labels = labels[permutation]\n",
    "    dm = dm[permutation]\n",
    "\n",
    "    trainset_conf = sorted_vcs[:m_training]\n",
    "    testset_conf = sorted_vcs[m_training:]\n",
    "\n",
    "    trainset_mat = dm[:m_training]\n",
    "    testset_mat = dm[m_training:]\n",
    "\n",
    "    counts = [\n",
    "        np.count_nonzero(labels[:m_training] == i)\n",
    "        for i in range(num_gammas)\n",
    "    ]\n",
    "\n",
    "    # if each label is represented by at least half of\n",
    "    # training set size / number of files\n",
    "    # we're good and we can stop permutating\n",
    "    if all(c > int(m_training / (2 * num_gammas)) for c in counts):\n",
    "        break\n",
    "\n",
    "print(\"Samples in training set: \", trainset_conf.shape[0])\n",
    "print(\"Samples in test set: \", testset_conf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b25743",
   "metadata": {},
   "source": [
    "## Variational Auto Encoder\n",
    "### Sampling Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811691a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSampling\u001b[39;00m(\u001b[43mlayers\u001b[49m\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      4\u001b[0m         z_mean, z_log_var \u001b[38;5;241m=\u001b[39m inputs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f97d3",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200a6e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(numpart, numpart, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(encoder_inputs)\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(numpart, numpart, 1))\n",
    "x = Conv2D(60, 3, padding=\"same\", activation=\"relu\")(encoder_inputs)\n",
    "x = Conv2D(50, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "conv_shape = K.int_shape(x)  # Shape of conv to be provided to decoder\n",
    "x = Flatten()(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(\n",
    "    encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\"\n",
    ")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a63cc",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4291ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "x = Dense(\n",
    "    conv_shape[1] * conv_shape[2] * conv_shape[3], activation=\"relu\"\n",
    ")(decoder_input)\n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "x = Conv2DTranspose(50, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(60, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "decoder_outputs = Conv2DTranspose(\n",
    "    1, 3, padding=\"same\", activation=\"sigmoid\", name=\"decoder_output\"\n",
    ")(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb77010",
   "metadata": {},
   "source": [
    "### VAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40334cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda = 0.00035\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # Extract dimensions excluding the first 'None' dimension\n",
    "            size = reconstruction.shape[1:]\n",
    "            # noise = np.random.normal(0, 0.1, size=size)\n",
    "            # reconstruction = reconstruction + noise\n",
    "\n",
    "            # Reshape data to match decoder output shape\n",
    "            data = tf.expand_dims(data, axis=-1)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.mean_squared_error(data, reconstruction)\n",
    "            )\n",
    "            kl_loss = -0.5 * (\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            total_loss = reconstruction_loss + reg_lambda * kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935300b",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))  # lower learning rate\n",
    "fit = vae.fit(trainset_mat, epochs=10, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f12cc1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m      2\u001b[0m fig, AX \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m6.0\u001b[39m))\n\u001b[1;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m AX[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"font.size\"] = 12\n",
    "fig, AX = plt.subplots(1, 2, figsize=(14, 6.0))\n",
    "ax = AX[0]\n",
    "ax.plot(fit.history[\"loss\"], label=\"MSE loss\", c=\"#78226B\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE loss\")\n",
    "ax.legend()\n",
    "ax = AX[1]\n",
    "ax.plot(fit.history[\"kl_loss\"], label=\"KL loss\", c=\"#FDD50B\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"KL loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b473",
   "metadata": {},
   "source": [
    "### Save model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab69446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf.keras.models.save_model(vae.encoder, \"saved_model/encoder\")\\ntf.keras.models.save_model(vae.decoder, \"saved_model/decoder\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tf.keras.models.save_model(vae.encoder, \"saved_model/encoder\")\n",
    "tf.keras.models.save_model(vae.decoder, \"saved_model/decoder\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a748b84",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "### Encoding and latent dimension\n",
    "Encode the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test = np.array(vae.encoder.predict(testset_mat))\n",
    "\n",
    "print(encoded_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e2ccf",
   "metadata": {},
   "source": [
    "Check clustering in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vis(vae, data, labels):\n",
    "    # prediction\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    transformed_data = pca.fit_transform(z_mean)\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    print(variance_ratio)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf33e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vis(vae, dm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd6121",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Decode test set and compare to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b21bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_test = np.array(decoder.predict(encoded_test[2, :, :])).reshape(-1, numpart, numpart)\n",
    "\n",
    "print(decoded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8300c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 42\n",
    "df = pd.DataFrame(testset_mat[ind])\n",
    "df2 = pd.DataFrame(decoded_test[ind])\n",
    "df3 = pd.DataFrame(abs(testset_mat[ind] - decoded_test[ind]))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(df), ax=axs[0])\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "sns.heatmap(pd.DataFrame(df2), ax=axs[1])\n",
    "axs[1].set_title('Reconstructed Data')\n",
    "\n",
    "sns.heatmap(pd.DataFrame(df3), ax=axs[2])\n",
    "axs[2].set_title('Difference')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(indm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2bfea",
   "metadata": {},
   "source": [
    "## Reconstruct Coordinates\n",
    "Extract points from distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_to_coordinates(distance_matrix):\n",
    "    # Get the number of points\n",
    "    n = distance_matrix.shape[0]\n",
    "\n",
    "    # Compute the Gram matrix\n",
    "    gram_matrix = -0.5 * (distance_matrix**2)\n",
    "\n",
    "    # Center the Gram matrix\n",
    "    gram_matrix_centered = (\n",
    "        gram_matrix\n",
    "        - np.mean(gram_matrix, axis=0)\n",
    "        - np.mean(gram_matrix, axis=1)[:, np.newaxis]\n",
    "        + np.mean(gram_matrix)\n",
    "    )\n",
    "\n",
    "    # Perform eigendecomposition of the centered Gram matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(gram_matrix_centered)\n",
    "\n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[indices]\n",
    "    eigenvectors = eigenvectors[:, indices]\n",
    "\n",
    "    # Extract the positive square root of eigenvalues\n",
    "    sqrt_eigenvalues = np.sqrt(np.maximum(eigenvalues, 0))\n",
    "\n",
    "    # Compute the coordinates of the points in 2D space\n",
    "    coordinates = eigenvectors[:, :2] * sqrt_eigenvalues[:2]\n",
    "\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983b7d6",
   "metadata": {},
   "source": [
    "Example with one configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd86a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93cd5091",
   "metadata": {},
   "source": [
    "Shift and rotate to match original data as good as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_points(points1, points2):\n",
    "    # Center the points by subtracting their means\n",
    "    centered_points1 = points1 - np.mean(points1, axis=0)\n",
    "    centered_points2 = points2 - np.mean(points2, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = centered_points2.T @ centered_points1\n",
    "\n",
    "    # Perform singular value decomposition (SVD)\n",
    "    U, _, Vt = np.linalg.svd(covariance_matrix)\n",
    "\n",
    "    # Calculate the optimal rotation matrix\n",
    "    rotation_matrix = Vt.T @ U.T\n",
    "\n",
    "    # Calculate the optimal translation vector\n",
    "    translation_vector = np.mean(points2, axis=0) - np.mean(\n",
    "        points1 @ rotation_matrix, axis=0\n",
    "    )\n",
    "\n",
    "    # Transform points1 using the estimated rotation and translation\n",
    "    transformed_points = points1 @ rotation_matrix + translation_vector\n",
    "\n",
    "    return transformed_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e4f8f",
   "metadata": {},
   "source": [
    "Example with one configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2b00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eab0a743",
   "metadata": {},
   "source": [
    "Apply to whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f08a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = align_points(coordinates, trainset_conf[1])\n",
    "ind = 1\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "l = np.sqrt(2)\n",
    "ax.scatter(\n",
    "    coordinates[:, 0] * l,\n",
    "    coordinates[:, 1] * l,\n",
    "    s=30,\n",
    "    c=\"#e63946\",\n",
    ")\n",
    "ax.scatter(\n",
    "    trainset_conf[ind, :, 0] * l,\n",
    "    trainset_conf[ind, :, 1] * l,\n",
    "    s=30,\n",
    "    c=\"#023e8a\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
